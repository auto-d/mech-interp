{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b98cbc35",
      "metadata": {
        "id": "b98cbc35"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This is a toy project for exploring mechanistic interpretability methods."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "713625dc",
      "metadata": {
        "id": "713625dc"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "SBmVRdtT34V4"
      },
      "id": "SBmVRdtT34V4",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A torch module for self attention. Informed by Andrej Karpathy's\n",
        "    mingpt project ( https://github.com/karpathy/minGPT) and a slightly embarassing\n",
        "    conversation with ChatGPT-5 ()\n",
        "\n",
        "    Our attention mechanism :\n",
        "    - A sequence of n-dimensional vectors is furnished, these are the embedded tokens\n",
        "      from the input\n",
        "    - For every vector as Q, we need to\n",
        "      - compute its similarity with all vectors in the sequence (we are not a 'causal'\n",
        "      model here, no shame at looking at the whole sequence if we're not generating!\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_dim=10, scale=False):\n",
        "        \"\"\"\n",
        "        Input dimensions are typically sharded across heads in multi-head attention.\n",
        "        We are aiming for simplicity and avoid this, usiing just a single 'head' with\n",
        "        the full input dimension.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.Q = nn.Linear(n_dim, n_dim)\n",
        "        self.K = nn.Linear(n_dim, n_dim)\n",
        "        self.V = nn.Linear(n_dim, n_dim)\n",
        "\n",
        "        self.n_dim = n_dim\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        We accept input in the shape of batch, seq length, model dimension.\n",
        "\n",
        "        Note we don't need any linear layers as output because we're only using a single\n",
        "        attention head. If we had more, we would need to map our heads back into the d_model\n",
        "        space with a linear layer.\n",
        "        \"\"\"\n",
        "        # (batch, seq_len, n_dim)\n",
        "\n",
        "        # Project our input into the query space (i.e. multiply by the query weights),\n",
        "        # do the same for the key space. Then apply our similarity operation (dot product\n",
        "        # by way of matmul) to yield an attention tensor.\n",
        "        q = self.Q(x)\n",
        "        k = self.K(x)\n",
        "        attn = torch.matmul(q, k.transpose(-2,-1))\n",
        "\n",
        "        # We optionally scale our attention values down to avoid them blasting off and saturating\n",
        "        # the softmax function (thereby destroying gradients during backprop). For tiny models,\n",
        "        # this is probably not an issue and so we allow omission to simplify the model.\n",
        "        if self.scale:\n",
        "            attn = attn / math.sqrt(self.n_dim)\n",
        "\n",
        "        # Now normalize our logits with softmax so we can scale the value vector based on the\n",
        "        # attention we are learning to pay to each respective token\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        v = self.V(x)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "\n",
        "        return out, attn\n"
      ],
      "metadata": {
        "id": "p2rWZzi84FFW"
      },
      "id": "p2rWZzi84FFW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "yYQKF59LEOg2"
      },
      "id": "yYQKF59LEOg2",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = torch.rand(1,3,3)"
      ],
      "metadata": {
        "id": "QUecM5E0EQU6"
      },
      "id": "QUecM5E0EQU6",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.shape"
      ],
      "metadata": {
        "id": "JOkMyXtiES_O",
        "outputId": "bafcf86e-0c97-4cb0-a2ce-52d5c1751e6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "JOkMyXtiES_O",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear = nn.Linear(3,3)"
      ],
      "metadata": {
        "id": "kZ05M2OxEgsC"
      },
      "id": "kZ05M2OxEgsC",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for p in linear.parameters():\n",
        "  print(p)"
      ],
      "metadata": {
        "id": "e3dPhfgNE4RL",
        "outputId": "04cd4759-5c80-40b9-a089-0ccac98bcf68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "e3dPhfgNE4RL",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0082, -0.0069,  0.1016],\n",
            "        [ 0.2007, -0.2645, -0.2846],\n",
            "        [-0.0276,  0.2386,  0.5372]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.5452, -0.2175, -0.2108], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = linear(test)"
      ],
      "metadata": {
        "id": "Nrj2WRv0ElYu"
      },
      "id": "Nrj2WRv0ElYu",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "id": "serKid_XEmiG",
        "outputId": "1c5c3548-9b62-4413-cb1e-2f7e44333a5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "serKid_XEmiG",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test.transpose(-1,0).shape"
      ],
      "metadata": {
        "id": "qAIzYPpsE2ie",
        "outputId": "ea7e788b-99cd-4a02-aa58-be46fdd73e55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qAIzYPpsE2ie",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 3, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6cxqL5V-Fm15"
      },
      "id": "6cxqL5V-Fm15",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}