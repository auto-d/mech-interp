{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b98cbc35",
      "metadata": {
        "id": "b98cbc35"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This is a toy project for exploring mechanistic interpretability methods. We train a small neural network to detect the presence of an ascii face in a string, then draw some conclusions about the network internals."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "Our challenge to the toy model will be to classify a text string as either containing an ASCII face or not.\n",
        "\n",
        "Dataset generation and unicode escape sequence cleaning and padding out the data courtesy of GPT-5 (see https://chatgpt.com/c/690e24c8-a990-832d-9711-8f0cb77e94a3)."
      ],
      "metadata": {
        "id": "7rQkWMfORIiy"
      },
      "id": "7rQkWMfORIiy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define dictionary of cute faces for our dataset. We'll interleave these among random characters from the same charset to present a challenge to the model."
      ],
      "metadata": {
        "id": "D49xsFw3c1eW"
      },
      "id": "D49xsFw3c1eW"
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "# Dictionary of horizontally-oriented ASCII faces (≈100 entries)\n",
        "ascii_faces = {\n",
        "    \"happy\": \"(^_^)\",\n",
        "    \"big_smile\": \"(^o^)\",\n",
        "    \"winky\": \"(^_~)\",\n",
        "    \"surprised\": \"(O_O)\",\n",
        "    \"disappointed\": \"(>_<)\",\n",
        "    \"cool\": \"(⌐■_■)\",\n",
        "    \"thinking\": \"(¬_¬)\",\n",
        "    \"angry\": \"(ಠ_ಠ)\",\n",
        "    \"lenny\": \"( ͡° ͜ʖ ͡°)\",\n",
        "    \"shrug\": \"¯\\\\_(ツ)_/¯\",\n",
        "    \"smirk\": \"(¬‿¬)\",\n",
        "    \"tired\": \"(‐_‐) zzz\",\n",
        "    \"love\": \"(♥_♥)\",\n",
        "    \"evil\": \"(>:) )\",\n",
        "    \"shock\": \"(°o°)\",\n",
        "    \"blush\": \"(^///^)\",\n",
        "    \"party\": \"(ﾉ◕ヮ◕)ﾉ*:･ﾟ✧\",\n",
        "    \"sunglasses\": \"(⌐■_■)\",\n",
        "    \"eyeroll\": \"(¬_ಠ)\",\n",
        "    \"confused\": \"(o_O)\",\n",
        "    \"awkward\": \"(._.)\",\n",
        "    \"victory\": \"＼(^o^)／\",\n",
        "    \"yay\": \"(^_^)v\",\n",
        "    \"hug\": \"(づ｡◕‿‿◕｡)づ\",\n",
        "    \"disgust\": \"(ಠ_ಠ ┌)\",\n",
        "    \"robot\": \"[¬º-°]¬\",\n",
        "    \"bang\": \"(ノಠ益ಠ)ノ彡┻━┻\",\n",
        "    \"wink_tongue\": \"(^<_<)\",\n",
        "    \"side_eye\": \"(>_> )\",\n",
        "    \"uhh\": \"(._. )\",\n",
        "    \"cry\": \"(;_;)\",\n",
        "    \"happy_tears\": \"(T_T)\",\n",
        "    \"cool_alt\": \"(▀̿Ĺ̯▀̿ ̿)\",\n",
        "    \"evil_grin\": \"(ʘ‿ʘ)\",\n",
        "    \"ache\": \"(x_x)\",\n",
        "    \"zombie\": \"(¬º-°)¬\",\n",
        "    \"joker\": \"(☭_☭)\",\n",
        "    \"pirate\": \"(☠_☠)\",\n",
        "    \"robot2\": \"(⌐■⁠_⁠■)\",\n",
        "    \"meh\": \"(-_-)\",\n",
        "    \"sleepy\": \"(‐ ‐)zzz\",\n",
        "    \"celebrate\": \"(ﾉ･ω･)ﾉﾞ\",\n",
        "    \"chuckle\": \"(＾ω＾)\",\n",
        "    \"grin\": \"(≧◡≦)\",\n",
        "    \"evil_smile\": \"(¬‿¬ )\",\n",
        "    \"sassy\": \"(¬_¬’)\",\n",
        "    \"tongue\": \"(:P)\",\n",
        "    \"groan\": \"(-｡-;)\",\n",
        "    \"hug2\": \"(づ￣ ³￣)づ\",\n",
        "    \"glasses\": \"(B^_^)B\",\n",
        "    \"victory2\": \"(^‿^✿)\",\n",
        "    \"cool3\": \"(⌐■_■)ﾉ\",\n",
        "    \"wut\": \"(°_°)\",\n",
        "    \"shock2\": \"(°◇°)\",\n",
        "    \"angry2\": \"(ಠ益ಠ)\",\n",
        "    \"eyeroll2\": \"(¬_¬)\",\n",
        "    \"blush2\": \"(^///^)\",\n",
        "    \"sad\": \"(T_T)\",\n",
        "    \"confused2\": \"(O_o)\",\n",
        "    \"surprised2\": \"(ʘ_ʘ)\",\n",
        "    \"wow\": \"(°д°)\",\n",
        "    \"smile2\": \"(^_−)☆\",\n",
        "    \"silly\": \"(-:)\",\n",
        "    \"yum\": \"(*≧ω≦)\",\n",
        "    \"facepalm\": \"(－‸ლ)\",\n",
        "    \"shush\": \"(☞ﾟヮﾟ)☞\",\n",
        "    \"raise_hand\": \"o(￣▽￣)o\",\n",
        "    \"highfive\": \"o/ (•‿•) \\\\o\",\n",
        "    \"zany\": \"(۞‿۞)\",\n",
        "    \"love2\": \"(♥‿♥)\",\n",
        "    \"grimace\": \"(>_<;)\",\n",
        "    \"smile3\": \"(◕‿◕)\",\n",
        "    \"mischief\": \"( ۞‿۞)\",\n",
        "    \"whistle\": \"(°▽°)ノ♪\",\n",
        "    \"cheer\": \"(ﾉ^_^)ﾉ\",\n",
        "    \"starstruck\": \"(✧ω✧)\",\n",
        "    \"cool5\": \"(⌐▨_▨)\",\n",
        "    \"ninja\": \"(ง •̀_•́)ง\",\n",
        "    \"worry\": \"(・_・;)\",\n",
        "    \"grumpy\": \"(－_－メ)\",\n",
        "    \"geek\": \"(ಠ‿↼)\",\n",
        "}"
      ],
      "metadata": {
        "id": "rLtQdphPRO1N"
      },
      "id": "rLtQdphPRO1N",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do some ChatGPT-powered cleaning of the handful of escaped characters that would deny us a char-> int mapping later."
      ],
      "metadata": {
        "id": "1RtnnMtIc-Vy"
      },
      "id": "1RtnnMtIc-Vy"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "from pprint import pprint\n",
        "\n",
        "# Characters we always strip (zero-widths, joiners, BOM, etc.)\n",
        "ZERO_WIDTH = {\n",
        "    '\\u200b', '\\u200c', '\\u200d', '\\u200e', '\\u200f',  # ZW*, RTL/LTR marks\n",
        "    '\\u2060', '\\u2061', '\\u2062', '\\u2063', '\\u2064',  # word joiner, etc.\n",
        "    '\\ufeff',                                          # BOM\n",
        "}\n",
        "# Optional substitutions to normalize lookalikes (tweak to taste)\n",
        "SUBS = {\n",
        "    '—': '-',  # em dash -> hyphen\n",
        "    '–': '-',  # en dash -> hyphen\n",
        "    '－': '-',  # fullwidth hyphen-minus -> hyphen\n",
        "    '·': '•',  # middle dot -> bullet (or flip if you prefer)\n",
        "    '﹏': '_', # fullwidth low line alt -> underscore\n",
        "}\n",
        "\n",
        "# Regex for literal \\uXXXX sequences (when strings are double-escaped)\n",
        "_U_ESC_RE = re.compile(r'\\\\u([0-9a-fA-F]{4})')\n",
        "\n",
        "def _decode_u_escapes(s: str) -> str:\n",
        "    # Replace literal \\uXXXX with the actual codepoint\n",
        "    return _U_ESC_RE.sub(lambda m: chr(int(m.group(1), 16)), s)\n",
        "\n",
        "def canonicalize(s: str, normalize_form=\"NFC\") -> str:\n",
        "    o = s\n",
        "    # 1) turn any literal \\uXXXX into real chars (if present)\n",
        "    s = _decode_u_escapes(s)\n",
        "    # 2) Unicode normalize (NFC keeps composed glyphs; NFKC is more aggressive)\n",
        "    s = unicodedata.normalize(normalize_form, s)\n",
        "    # 3) strip zero-width/control-ish characters\n",
        "    s = ''.join(ch for ch in s if ch not in ZERO_WIDTH and not unicodedata.category(ch).startswith('C'))\n",
        "    # 4) apply substitutions\n",
        "    s = ''.join(SUBS.get(ch, ch) for ch in s)\n",
        "\n",
        "    if o != s:\n",
        "        print(f\"Converted escaped sequence '{o}' to '{s}'\")\n",
        "    return s\n",
        "\n",
        "def canonicalize_faces(dct: dict) -> dict:\n",
        "    return {k: canonicalize(v) for k, v in dct.items()}"
      ],
      "metadata": {
        "id": "3jZmLtOjTfgL"
      },
      "id": "3jZmLtOjTfgL",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Strip out escaped sequences and swap in something that won't blow up my vectorization\n",
        "clean_faces = canonicalize_faces(ascii_faces)\n",
        "\n",
        "# Grab the charset of the\n",
        "face_charset = set()\n",
        "for face in clean_faces.values():\n",
        "    face_charset.update(face)\n",
        "face_charset = ''.join(sorted(face_charset))\n",
        "\n",
        "print(\"Face character set:\", face_charset)  # helpful for debug"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybZo3-K_T7yQ",
        "outputId": "f4b1475f-0608-4f73-848b-614475eb7913"
      },
      "id": "ybZo3-K_T7yQ",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted escaped sequence '(⌐■⁠_⁠■)' to '(⌐■_■)'\n",
            "Converted escaped sequence '(－‸ლ)' to '(-‸ლ)'\n",
            "Converted escaped sequence '(－_－メ)' to '(-_-メ)'\n",
            "Face character set:  ()*-./:;<>BOPT[\\]^_ovxz~¬¯°³ºĹʖʘ̯̀́̿͜͡ωд۞ಠงლ‐’•‸‿↼−≦≧⌐━┌┻▀■▨▽◇◕◡☆☞☠☭♥♪✧✿づツノメヮ・彡益／＼＾｡･ﾉﾞﾟ￣\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define some ChatGPT-sourced routines for randomly sampling our cleaned face dict."
      ],
      "metadata": {
        "id": "kmFiskFMdMtC"
      },
      "id": "kmFiskFMdMtC"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_random_face():\n",
        "    \"\"\"Return a random ASCII-face from the dictionary.\"\"\"\n",
        "    return random.choice(list(clean_faces.values()))\n",
        "\n",
        "def get_random_nonface(min_len=3, max_len=10):\n",
        "    \"\"\"\n",
        "    Generate a random string of characters drawn only from face_charset,\n",
        "    but arranged in a way that likely doesn’t look like a face.\n",
        "    \"\"\"\n",
        "    length = random.randint(min_len, max_len)\n",
        "    return ''.join(random.choice(face_charset) for _ in range(length))\n",
        "\n",
        "def sample_face(face_prob=0.5):\n",
        "    \"\"\"Sample either a face (positive) or a random non-face (negative).\"\"\"\n",
        "    if random.random() < face_prob:\n",
        "        return get_random_face(), True\n",
        "    else:\n",
        "        return get_random_nonface(), False\n",
        "\n",
        "def generate_dataset(n_samples, face_prob=0.5):\n",
        "    \"\"\"Generate a list of (text, label) pairs.\"\"\"\n",
        "    return [ sample_face(face_prob) for _ in range(n_samples) ]\n"
      ],
      "metadata": {
        "id": "IsuwjDHmVhAB"
      },
      "id": "IsuwjDHmVhAB",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now randomly select a few faces and non-faces along with their label to validate the strategy. Then emit a dataset-worthy array of same."
      ],
      "metadata": {
        "id": "hoeliXWddY6S"
      },
      "id": "hoeliXWddY6S"
    },
    {
      "cell_type": "code",
      "source": [
        "# Test our dataset generator\n",
        "for _ in range(10):\n",
        "    seq, label = sample_face(face_prob=0.5)\n",
        "    print(f\"{seq} => {'FACE' if label else 'NON-FACE'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmwtRiMHRfEy",
        "outputId": "96409d1d-12ef-477a-ba44-dc7e2cf29d31"
      },
      "id": "AmwtRiMHRfEy",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(x_x) => FACE\n",
            "(⌐■_■) => FACE\n",
            "^Ĺヮ−B> => NON-FACE\n",
            "▽メ\\;:̯  => NON-FACE\n",
            "･━́ω ♥* => NON-FACE\n",
            "(≧◡≦) => FACE\n",
            "(x_x) => FACE\n",
            "(¬‿¬ ) => FACE\n",
            "۞・‸́ => NON-FACE\n",
            "(^///^) => FACE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a dataset of faces and non-faces\n",
        "seqs = []\n",
        "labels = []\n",
        "for _ in range(1000):\n",
        "  seq, label = sample_face(face_prob=0.5)\n",
        "  seqs.append(seq)\n",
        "  labels.append(label)"
      ],
      "metadata": {
        "id": "-GvtEbhjRgvL"
      },
      "id": "-GvtEbhjRgvL",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we need to get to a fixed-width for our input vector to the model we'll build. Sprinkle some GPT-5 sauce to define functions that map our characters into a relatively compact integer space (avoids super sparse values we'd otherwise get if we used the unicode value) and emit a padded variant of our dataset (fixed sequence length)."
      ],
      "metadata": {
        "id": "Mj_iP2TGdT_-"
      },
      "id": "Mj_iP2TGdT_-"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_global_char_mapping(all_characters):\n",
        "    \"\"\"\n",
        "    Build integer mappings for every unique Unicode character in the dataset.\n",
        "\n",
        "    Parameters:\n",
        "        all_characters (Iterable[str]): A set or list of all unique characters\n",
        "                                        you expect to encounter (faces + non-faces).\n",
        "    Returns:\n",
        "        tuple(dict, dict): (char_to_int, int_to_char)\n",
        "    \"\"\"\n",
        "    # Sort for deterministic ordering (important for reproducibility)\n",
        "    sorted_chars = sorted(all_characters)\n",
        "\n",
        "    # Assign a unique integer ID to each character\n",
        "    char_to_int = {ch: idx for idx, ch in enumerate(sorted_chars)}\n",
        "    int_to_char = {idx: ch for ch, idx in char_to_int.items()}\n",
        "\n",
        "    return char_to_int, int_to_char\n",
        "\n",
        "def encode_strings(strings, char2int):\n",
        "    \"\"\"\n",
        "    Convert a list of strings into integer-based representations\n",
        "    using the provided char2int mapping.\n",
        "    Unknown characters are ignored or mapped to 0 if desired.\n",
        "    Returns a list of lists of integers.\n",
        "    \"\"\"\n",
        "    encoded = []\n",
        "    for s in strings:\n",
        "        encoded.append([char2int.get(ch, 0) for ch in s])\n",
        "    return encoded\n",
        "\n",
        "def random_pad_sequences(encoded_strings, pad_value=0):\n",
        "    \"\"\"\n",
        "    Randomly pad each integer sequence (left or right) so all have the same length.\n",
        "    pad_value is used for padding.\n",
        "    Returns a new list of equal-length sequences.\n",
        "    \"\"\"\n",
        "    # 1. Determine max length\n",
        "    max_len = max(len(seq) for seq in encoded_strings)\n",
        "    padded = []\n",
        "\n",
        "    for seq in encoded_strings:\n",
        "        pad_len = max_len - len(seq)\n",
        "        if pad_len == 0:\n",
        "            padded.append(seq)\n",
        "            continue\n",
        "\n",
        "        # Randomly choose left or right padding\n",
        "        if random.random() < 0.5:\n",
        "            # left pad\n",
        "            new_seq = [pad_value] * pad_len + seq\n",
        "        else:\n",
        "            # right pad\n",
        "            new_seq = seq + [pad_value] * pad_len\n",
        "\n",
        "        padded.append(new_seq)\n",
        "\n",
        "    return padded, max_len"
      ],
      "metadata": {
        "id": "5V1HSjq5Sid_"
      },
      "id": "5V1HSjq5Sid_",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a mapping and test the encoding to see a string and it's integer value:"
      ],
      "metadata": {
        "id": "Hz9YkTUheNjA"
      },
      "id": "Hz9YkTUheNjA"
    },
    {
      "cell_type": "code",
      "source": [
        "char2int, int2char = build_global_char_mapping(face_charset)\n",
        "\n",
        "test = seqs[1]\n",
        "print(test)\n",
        "encode_strings([test], char2int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_JIQ6s2bAq0",
        "outputId": "accd5c50-8fa8-433e-d082-2d6b5c95552d"
      },
      "id": "b_JIQ6s2bAq0",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O☭-\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[12, 68, 4]]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can properly encode our strings and pad the result randomly to achieve fixed-width and positional diversity. We'll scale and recast these as floats to prepare for modeling."
      ],
      "metadata": {
        "id": "-s-Bqzb-edNV"
      },
      "id": "-s-Bqzb-edNV"
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = encode_strings(seqs, char2int)\n",
        "padded, seq_len = random_pad_sequences(encoded)"
      ],
      "metadata": {
        "id": "vO62-nL7aIqW"
      },
      "id": "vO62-nL7aIqW",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.array(padded[0])/90"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNkMwM16kKIs",
        "outputId": "95e4c02b-7f4b-44f0-805f-7380b47ed99e"
      },
      "id": "pNkMwM16kKIs",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.78888889, 0.76666667, 0.21111111,\n",
              "       0.02222222, 0.12222222])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_length = len(char2int)\n",
        "X = [ np.array(seq) / vocab_length for seq in padded]\n",
        "X = np.array(X)\n",
        "y = labels"
      ],
      "metadata": {
        "id": "D26vq9jre_Ag"
      },
      "id": "D26vq9jre_Ag",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7pfnZbBllTg",
        "outputId": "916a3a6e-ac3e-404f-89bd-05be47b05e61"
      },
      "id": "p7pfnZbBllTg",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzgjHKqnfAev",
        "outputId": "51223ca2-8ebb-4708-bc37-74a6e7e2693a"
      },
      "id": "CzgjHKqnfAev",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.78888889, 0.76666667, 0.21111111,\n",
              "        0.02222222, 0.12222222],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.13333333,\n",
              "        0.75555556, 0.04444444],\n",
              "       [0.        , 0.        , 0.53333333, 0.33333333, 0.31111111,\n",
              "        0.04444444, 0.32222222, 0.        , 0.43333333, 0.42222222,\n",
              "        0.04444444, 0.34444444],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.01111111, 0.6       , 0.65555556, 0.21111111, 0.65555556,\n",
              "        0.02222222, 0.95555556],\n",
              "       [0.27777778, 0.66666667, 0.15555556, 0.78888889, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDBE780qfCkQ",
        "outputId": "15f2283e-9806-4343-894d-3579ab6e1349"
      },
      "id": "tDBE780qfCkQ",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[False, False, False, True, False]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture"
      ],
      "metadata": {
        "id": "y85A_CpacQXJ"
      },
      "id": "y85A_CpacQXJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now need to build a model that will accept out weird sequence and try to classify it. We opt for a wickedly pared down variant of the transformer."
      ],
      "metadata": {
        "id": "7FUVHNOseqJv"
      },
      "id": "7FUVHNOseqJv"
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "SBmVRdtT34V4"
      },
      "id": "SBmVRdtT34V4",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A toy torch module for self attention. Informed by Andrej Karpathy's mingpt project\n",
        "    (https://github.com/karpathy/minGPT) and a slightly embarassing conversation with\n",
        "    ChatGPT-5 (https://chatgpt.com/share/690e21fa-e718-8004-860a-45109a95c291)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_dim=10, scale=False):\n",
        "        \"\"\"\n",
        "        Input dimensions are typically sharded across heads in multi-head attention.\n",
        "        We are aiming for simplicity and avoid this, usiing just a single 'head' with\n",
        "        the full input dimension.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.Q = nn.Linear(n_dim, n_dim)\n",
        "        self.K = nn.Linear(n_dim, n_dim)\n",
        "        self.V = nn.Linear(n_dim, n_dim)\n",
        "\n",
        "        self.n_dim = n_dim\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        We accept input in the shape of batch, seq length, model dimension.\n",
        "\n",
        "        Note we don't need any linear layers as output because we're only using a single\n",
        "        attention head. If we had more, we would need to map our heads back into the d_model\n",
        "        space with a linear layer.\n",
        "        \"\"\"\n",
        "\n",
        "        # Project our input into the query space (i.e. multiply by the query weights),\n",
        "        # do the same for the key vectors. Then apply our similarity operation (dot product\n",
        "        # by way of matmul) to yield an attention tensor.\n",
        "        q = self.Q(x)\n",
        "        k = self.K(x)\n",
        "        attn = torch.matmul(q, k.transpose(-2,-1))\n",
        "\n",
        "        # We optionally scale our attention values down to avoid them blasting off and saturating\n",
        "        # the softmax function (thereby destroying gradients during backprop). For tiny models,\n",
        "        # this is probably not an issue and so we allow omission to simplify the model.\n",
        "        if self.scale:\n",
        "            attn = attn / math.sqrt(self.n_dim)\n",
        "\n",
        "        # Now normalize our logits with softmax so we can scale the value vector based on the\n",
        "        # attention we are learning to pay to each respective token\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        v = self.V(x)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "\n",
        "        return out, attn\n"
      ],
      "metadata": {
        "id": "p2rWZzi84FFW"
      },
      "id": "p2rWZzi84FFW",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FaceClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_dim):\n",
        "      \"\"\"\n",
        "      Define a model that accepts a sequen\n",
        "       ce and predicts whether it contains a face.\n",
        "      \"\"\"\n",
        "      super().__init__()\n",
        "\n",
        "      # Give the model a scratch pad to decompose our characters\n",
        "      self.embedder = nn.Linear(1, n_dim)\n",
        "      self.attn = SelfAttention(n_dim=n_dim, scale=True)\n",
        "      self.fc = nn.Linear(n_dim, 1)\n",
        "      self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, X):\n",
        "      \"\"\"\n",
        "      Do a forward pass through the network, applying attention and emitting a class probability.\n",
        "      We accept a sequence of single values (representing the respeective character), of\n",
        "      shape (batch, seq, 1).\n",
        "      \"\"\"\n",
        "      assert(X.shape[-1] == 1)\n",
        "\n",
        "      X = self.embedder(X)\n",
        "      X = self.attn(X)\n",
        "      X = self.fc(X)\n",
        "      X = self.sigmoid(X)\n",
        "\n",
        "      return X"
      ],
      "metadata": {
        "id": "B1vWSfSafGpK"
      },
      "id": "B1vWSfSafGpK",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, X_train, y_train, epochs=100):\n",
        "    \"\"\"\n",
        "    Train the face classifier\n",
        "    \"\"\"\n",
        "\n",
        "    # Use a fancy optimizer to help us converge, default learning rate\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    # Binary cross-entropy loss calculator for our binary classification task\n",
        "    loss_fn = nn.BCELoss()\n",
        "\n",
        "    loss_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # Training mode, ensure gradients are tracked, etc.\n",
        "        model.train()\n",
        "\n",
        "        # Forward pass\n",
        "        out, _ = model(X_train)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(out, y_train)\n",
        "\n",
        "        # Backprop\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Calculate gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Apply those gradients to inch, bound, fly towards lower loss\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_history.append(loss.item())"
      ],
      "metadata": {
        "id": "s_bNZNHuiApy"
      },
      "id": "s_bNZNHuiApy",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling"
      ],
      "metadata": {
        "id": "cthtKjHIhtUz"
      },
      "id": "cthtKjHIhtUz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's split up our data and train the model!"
      ],
      "metadata": {
        "id": "bk7q9_AzhvjO"
      },
      "id": "bk7q9_AzhvjO"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "H4nL30GQhmAd"
      },
      "id": "H4nL30GQhmAd",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FaceClassifier(n_dim=seq_len)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baN7xKRohgIV",
        "outputId": "9c9844b5-e7ea-4f9f-aa71-e4ee5628a098"
      },
      "id": "baN7xKRohgIV",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FaceClassifier(\n",
            "  (embedder): Linear(in_features=1, out_features=12, bias=True)\n",
            "  (attn): SelfAttention(\n",
            "    (Q): Linear(in_features=12, out_features=12, bias=True)\n",
            "    (K): Linear(in_features=12, out_features=12, bias=True)\n",
            "    (V): Linear(in_features=12, out_features=12, bias=True)\n",
            "  )\n",
            "  (fc): Linear(in_features=12, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, X_train, y_train, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "ouPzsDzio1Rg",
        "outputId": "956dde5b-c07c-4027-e41a-706a9f8e645b"
      },
      "id": "ouPzsDzio1Rg",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3830930334.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1367993188.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, X_train, y_train, epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3380137586.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \"\"\"\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8J3yyB4QGrFd"
      },
      "id": "8J3yyB4QGrFd",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}