{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b98cbc35",
      "metadata": {
        "id": "b98cbc35"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This is a toy project for exploring mechanistic interpretability methods. We train a small neural network to detect the presence of an ascii face in a string, then draw some conclusions about the network internals."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7rQkWMfORIiy",
      "metadata": {
        "id": "7rQkWMfORIiy"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Our challenge to the toy model will be to classify a text string as either containing an ASCII face or not.\n",
        "\n",
        "Dataset generation and unicode escape sequence cleaning and padding out the data courtesy of GPT-5 (see https://chatgpt.com/c/690e24c8-a990-832d-9711-8f0cb77e94a3)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D49xsFw3c1eW",
      "metadata": {
        "id": "D49xsFw3c1eW"
      },
      "source": [
        "Define dictionary of cute faces for our dataset. We'll interleave these among random characters from the same charset to present a challenge to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "rLtQdphPRO1N",
      "metadata": {
        "id": "rLtQdphPRO1N"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "# Dictionary of horizontally-oriented ASCII faces (≈100 entries)\n",
        "ascii_faces = {\n",
        "    \"happy\": \"(^_^)\",\n",
        "    \"big_smile\": \"(^o^)\",\n",
        "    \"winky\": \"(^_~)\",\n",
        "    \"surprised\": \"(O_O)\",\n",
        "    \"disappointed\": \"(>_<)\",\n",
        "    \"cool\": \"(⌐■_■)\",\n",
        "    \"thinking\": \"(¬_¬)\",\n",
        "    \"angry\": \"(ಠ_ಠ)\",\n",
        "    \"lenny\": \"( ͡° ͜ʖ ͡°)\",\n",
        "    \"shrug\": \"¯\\\\_(ツ)_/¯\",\n",
        "    \"smirk\": \"(¬‿¬)\",\n",
        "    \"tired\": \"(‐_‐) zzz\",\n",
        "    \"love\": \"(♥_♥)\",\n",
        "    \"evil\": \"(>:) )\",\n",
        "    \"shock\": \"(°o°)\",\n",
        "    \"blush\": \"(^///^)\",\n",
        "    \"party\": \"(ﾉ◕ヮ◕)ﾉ*:･ﾟ✧\",\n",
        "    \"sunglasses\": \"(⌐■_■)\",\n",
        "    \"eyeroll\": \"(¬_ಠ)\",\n",
        "    \"confused\": \"(o_O)\",\n",
        "    \"awkward\": \"(._.)\",\n",
        "    \"victory\": \"＼(^o^)／\",\n",
        "    \"yay\": \"(^_^)v\",\n",
        "    \"hug\": \"(づ｡◕‿‿◕｡)づ\",\n",
        "    \"disgust\": \"(ಠ_ಠ ┌)\",\n",
        "    \"robot\": \"[¬º-°]¬\",\n",
        "    \"bang\": \"(ノಠ益ಠ)ノ彡┻━┻\",\n",
        "    \"wink_tongue\": \"(^<_<)\",\n",
        "    \"side_eye\": \"(>_> )\",\n",
        "    \"uhh\": \"(._. )\",\n",
        "    \"cry\": \"(;_;)\",\n",
        "    \"happy_tears\": \"(T_T)\",\n",
        "    \"cool_alt\": \"(▀̿Ĺ̯▀̿ ̿)\",\n",
        "    \"evil_grin\": \"(ʘ‿ʘ)\",\n",
        "    \"ache\": \"(x_x)\",\n",
        "    \"zombie\": \"(¬º-°)¬\",\n",
        "    \"joker\": \"(☭_☭)\",\n",
        "    \"pirate\": \"(☠_☠)\",\n",
        "    \"robot2\": \"(⌐■⁠_⁠■)\",\n",
        "    \"meh\": \"(-_-)\",\n",
        "    \"sleepy\": \"(‐ ‐)zzz\",\n",
        "    \"celebrate\": \"(ﾉ･ω･)ﾉﾞ\",\n",
        "    \"chuckle\": \"(＾ω＾)\",\n",
        "    \"grin\": \"(≧◡≦)\",\n",
        "    \"evil_smile\": \"(¬‿¬ )\",\n",
        "    \"sassy\": \"(¬_¬’)\",\n",
        "    \"tongue\": \"(:P)\",\n",
        "    \"groan\": \"(-｡-;)\",\n",
        "    \"hug2\": \"(づ￣ ³￣)づ\",\n",
        "    \"glasses\": \"(B^_^)B\",\n",
        "    \"victory2\": \"(^‿^✿)\",\n",
        "    \"cool3\": \"(⌐■_■)ﾉ\",\n",
        "    \"wut\": \"(°_°)\",\n",
        "    \"shock2\": \"(°◇°)\",\n",
        "    \"angry2\": \"(ಠ益ಠ)\",\n",
        "    \"eyeroll2\": \"(¬_¬)\",\n",
        "    \"blush2\": \"(^///^)\",\n",
        "    \"sad\": \"(T_T)\",\n",
        "    \"confused2\": \"(O_o)\",\n",
        "    \"surprised2\": \"(ʘ_ʘ)\",\n",
        "    \"wow\": \"(°д°)\",\n",
        "    \"smile2\": \"(^_−)☆\",\n",
        "    \"silly\": \"(-:)\",\n",
        "    \"yum\": \"(*≧ω≦)\",\n",
        "    \"facepalm\": \"(－‸ლ)\",\n",
        "    \"shush\": \"(☞ﾟヮﾟ)☞\",\n",
        "    \"raise_hand\": \"o(￣▽￣)o\",\n",
        "    \"highfive\": \"o/ (•‿•) \\\\o\",\n",
        "    \"zany\": \"(۞‿۞)\",\n",
        "    \"love2\": \"(♥‿♥)\",\n",
        "    \"grimace\": \"(>_<;)\",\n",
        "    \"smile3\": \"(◕‿◕)\",\n",
        "    \"mischief\": \"( ۞‿۞)\",\n",
        "    \"whistle\": \"(°▽°)ノ♪\",\n",
        "    \"cheer\": \"(ﾉ^_^)ﾉ\",\n",
        "    \"starstruck\": \"(✧ω✧)\",\n",
        "    \"cool5\": \"(⌐▨_▨)\",\n",
        "    \"ninja\": \"(ง •̀_•́)ง\",\n",
        "    \"worry\": \"(・_・;)\",\n",
        "    \"grumpy\": \"(－_－メ)\",\n",
        "    \"geek\": \"(ಠ‿↼)\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1RtnnMtIc-Vy",
      "metadata": {
        "id": "1RtnnMtIc-Vy"
      },
      "source": [
        "Do some ChatGPT-powered cleaning of the handful of escaped characters that would deny us a char-> int mapping later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3jZmLtOjTfgL",
      "metadata": {
        "id": "3jZmLtOjTfgL"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "from pprint import pprint\n",
        "\n",
        "# Characters we always strip (zero-widths, joiners, BOM, etc.)\n",
        "ZERO_WIDTH = {\n",
        "    '\\u200b', '\\u200c', '\\u200d', '\\u200e', '\\u200f',  # ZW*, RTL/LTR marks\n",
        "    '\\u2060', '\\u2061', '\\u2062', '\\u2063', '\\u2064',  # word joiner, etc.\n",
        "    '\\ufeff',                                          # BOM\n",
        "}\n",
        "# Optional substitutions to normalize lookalikes (tweak to taste)\n",
        "SUBS = {\n",
        "    '—': '-',  # em dash -> hyphen\n",
        "    '–': '-',  # en dash -> hyphen\n",
        "    '－': '-',  # fullwidth hyphen-minus -> hyphen\n",
        "    '·': '•',  # middle dot -> bullet (or flip if you prefer)\n",
        "    '﹏': '_', # fullwidth low line alt -> underscore\n",
        "}\n",
        "\n",
        "# Regex for literal \\uXXXX sequences (when strings are double-escaped)\n",
        "_U_ESC_RE = re.compile(r'\\\\u([0-9a-fA-F]{4})')\n",
        "\n",
        "def _decode_u_escapes(s: str) -> str:\n",
        "    # Replace literal \\uXXXX with the actual codepoint\n",
        "    return _U_ESC_RE.sub(lambda m: chr(int(m.group(1), 16)), s)\n",
        "\n",
        "def canonicalize(s: str, normalize_form=\"NFC\") -> str:\n",
        "    o = s\n",
        "    # 1) turn any literal \\uXXXX into real chars (if present)\n",
        "    s = _decode_u_escapes(s)\n",
        "    # 2) Unicode normalize (NFC keeps composed glyphs; NFKC is more aggressive)\n",
        "    s = unicodedata.normalize(normalize_form, s)\n",
        "    # 3) strip zero-width/control-ish characters\n",
        "    s = ''.join(ch for ch in s if ch not in ZERO_WIDTH and not unicodedata.category(ch).startswith('C'))\n",
        "    # 4) apply substitutions\n",
        "    s = ''.join(SUBS.get(ch, ch) for ch in s)\n",
        "\n",
        "    if o != s:\n",
        "        print(f\"Converted escaped sequence '{o}' to '{s}'\")\n",
        "    return s\n",
        "\n",
        "def canonicalize_faces(dct: dict) -> dict:\n",
        "    return {k: canonicalize(v) for k, v in dct.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ybZo3-K_T7yQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybZo3-K_T7yQ",
        "outputId": "7ee811b3-08fb-43d3-c637-6b76bb0b63ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted escaped sequence '(⌐■⁠_⁠■)' to '(⌐■_■)'\n",
            "Converted escaped sequence '(－‸ლ)' to '(-‸ლ)'\n",
            "Converted escaped sequence '(－_－メ)' to '(-_-メ)'\n",
            "Face character set:  ()*-./:;<>BOPT[\\]^_ovxz~¬¯°³ºĹʖʘ̯̀́̿͜͡ωд۞ಠงლ‐’•‸‿↼−≦≧⌐━┌┻▀■▨▽◇◕◡☆☞☠☭♥♪✧✿づツノメヮ・彡益／＼＾｡･ﾉﾞﾟ￣\n"
          ]
        }
      ],
      "source": [
        "# Strip out escaped sequences and swap in something that won't blow up my vectorization\n",
        "clean_faces = canonicalize_faces(ascii_faces)\n",
        "\n",
        "# Grab the charset of the\n",
        "face_charset = set()\n",
        "for face in clean_faces.values():\n",
        "    face_charset.update(face)\n",
        "face_charset = ''.join(sorted(face_charset))\n",
        "\n",
        "print(\"Face character set:\", face_charset)  # helpful for debug"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kmFiskFMdMtC",
      "metadata": {
        "id": "kmFiskFMdMtC"
      },
      "source": [
        "Define some ChatGPT-sourced routines for randomly sampling our cleaned face dict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "IsuwjDHmVhAB",
      "metadata": {
        "id": "IsuwjDHmVhAB"
      },
      "outputs": [],
      "source": [
        "def get_random_face():\n",
        "    \"\"\"Return a random ASCII-face from the dictionary.\"\"\"\n",
        "    return random.choice(list(clean_faces.values()))\n",
        "\n",
        "def get_random_nonface(min_len=3, max_len=10):\n",
        "    \"\"\"\n",
        "    Generate a random string of characters drawn only from face_charset,\n",
        "    but arranged in a way that likely doesn’t look like a face.\n",
        "    \"\"\"\n",
        "    length = random.randint(min_len, max_len)\n",
        "    return ''.join(random.choice(face_charset) for _ in range(length))\n",
        "\n",
        "def sample_face(face_prob=0.5):\n",
        "    \"\"\"Sample either a face (positive) or a random non-face (negative).\"\"\"\n",
        "    if random.random() < face_prob:\n",
        "        return get_random_face(), True\n",
        "    else:\n",
        "        return get_random_nonface(), False\n",
        "\n",
        "def generate_dataset(n_samples, face_prob=0.5):\n",
        "    \"\"\"Generate a list of (text, label) pairs.\"\"\"\n",
        "    return [ sample_face(face_prob) for _ in range(n_samples) ]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hoeliXWddY6S",
      "metadata": {
        "id": "hoeliXWddY6S"
      },
      "source": [
        "Now randomly select a few faces and non-faces along with their label to validate the strategy. Then emit a dataset-worthy array of same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "AmwtRiMHRfEy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmwtRiMHRfEy",
        "outputId": "01adbb90-9655-4c60-8bba-78147ff14500"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(づ￣ ³￣)づ => FACE\n",
            "(°д°) => FACE\n",
            "￣☭\\z͜▽▀‸ => NON-FACE\n",
            "(¬º-°)¬ => FACE\n",
            "(°▽°)ノ♪ => FACE\n",
            "º■~ツ*ლ => NON-FACE\n",
            "／↼:･) => NON-FACE\n",
            "ʖ■◕♪°z[́♪◇ => NON-FACE\n",
            "‿‐°> => NON-FACE\n",
            "(♥_♥) => FACE\n"
          ]
        }
      ],
      "source": [
        "# Test our dataset generator\n",
        "for _ in range(10):\n",
        "    seq, label = sample_face(face_prob=0.5)\n",
        "    print(f\"{seq} => {'FACE' if label else 'NON-FACE'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "-GvtEbhjRgvL",
      "metadata": {
        "id": "-GvtEbhjRgvL"
      },
      "outputs": [],
      "source": [
        "# Generate a dataset of faces and non-faces\n",
        "seqs = []\n",
        "labels = []\n",
        "for _ in range(1000):\n",
        "  seq, label = sample_face(face_prob=0.5)\n",
        "  seqs.append(seq)\n",
        "  labels.append(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Mj_iP2TGdT_-",
      "metadata": {
        "id": "Mj_iP2TGdT_-"
      },
      "source": [
        "Finally, we need to get to a fixed-width for our input vector to the model we'll build. Sprinkle some GPT-5 sauce to define functions that map our characters into a relatively compact integer space (avoids super sparse values we'd otherwise get if we used the unicode value) and emit a padded variant of our dataset (fixed sequence length)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5V1HSjq5Sid_",
      "metadata": {
        "id": "5V1HSjq5Sid_"
      },
      "outputs": [],
      "source": [
        "def build_global_char_mapping(all_characters):\n",
        "    \"\"\"\n",
        "    Build integer mappings for every unique Unicode character in the dataset.\n",
        "\n",
        "    Parameters:\n",
        "        all_characters (Iterable[str]): A set or list of all unique characters\n",
        "                                        you expect to encounter (faces + non-faces).\n",
        "    Returns:\n",
        "        tuple(dict, dict): (char_to_int, int_to_char)\n",
        "    \"\"\"\n",
        "    # Sort for deterministic ordering (important for reproducibility)\n",
        "    sorted_chars = sorted(all_characters)\n",
        "\n",
        "    # Assign a unique integer ID to each character\n",
        "    char_to_int = {ch: idx for idx, ch in enumerate(sorted_chars)}\n",
        "    int_to_char = {idx: ch for ch, idx in char_to_int.items()}\n",
        "\n",
        "    return char_to_int, int_to_char\n",
        "\n",
        "def encode_strings(strings, char2int):\n",
        "    \"\"\"\n",
        "    Convert a list of strings into integer-based representations\n",
        "    using the provided char2int mapping.\n",
        "    Unknown characters are ignored or mapped to 0 if desired.\n",
        "    Returns a list of lists of integers.\n",
        "    \"\"\"\n",
        "    encoded = []\n",
        "    for s in strings:\n",
        "        encoded.append([char2int.get(ch, 0) for ch in s])\n",
        "    return encoded\n",
        "\n",
        "def random_pad_sequences(encoded_strings, pad_value=0):\n",
        "    \"\"\"\n",
        "    Randomly pad each integer sequence (left or right) so all have the same length.\n",
        "    pad_value is used for padding.\n",
        "    Returns a new list of equal-length sequences.\n",
        "    \"\"\"\n",
        "    # 1. Determine max length\n",
        "    max_len = max(len(seq) for seq in encoded_strings)\n",
        "    padded = []\n",
        "\n",
        "    for seq in encoded_strings:\n",
        "        pad_len = max_len - len(seq)\n",
        "        if pad_len == 0:\n",
        "            padded.append(seq)\n",
        "            continue\n",
        "\n",
        "        # Randomly choose left or right padding\n",
        "        if random.random() < 0.5:\n",
        "            # left pad\n",
        "            new_seq = [pad_value] * pad_len + seq\n",
        "        else:\n",
        "            # right pad\n",
        "            new_seq = seq + [pad_value] * pad_len\n",
        "\n",
        "        padded.append(new_seq)\n",
        "\n",
        "    return padded, max_len"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Hz9YkTUheNjA",
      "metadata": {
        "id": "Hz9YkTUheNjA"
      },
      "source": [
        "Build a mapping and test the encoding to see a string and it's integer value:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b_JIQ6s2bAq0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_JIQ6s2bAq0",
        "outputId": "9cbb83bd-a602-4a76-ad84-a0065832be76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(¬_ಠ)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[[1, 25, 19, 42, 2]]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "char2int, int2char = build_global_char_mapping(face_charset)\n",
        "\n",
        "test = seqs[1]\n",
        "print(test)\n",
        "encode_strings([test], char2int)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-s-Bqzb-edNV",
      "metadata": {
        "id": "-s-Bqzb-edNV"
      },
      "source": [
        "Now we can properly encode our strings and pad the result randomly to achieve fixed-width and positional diversity. We'll scale and recast these as floats to prepare for modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vO62-nL7aIqW",
      "metadata": {
        "id": "vO62-nL7aIqW"
      },
      "outputs": [],
      "source": [
        "encoded = encode_strings(seqs, char2int)\n",
        "padded, seq_len = random_pad_sequences(encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "D26vq9jre_Ag",
      "metadata": {
        "id": "D26vq9jre_Ag"
      },
      "outputs": [],
      "source": [
        "vocab_length = len(char2int)\n",
        "X = torch.tensor(padded, dtype=torch.int32)\n",
        "y = torch.tensor(labels, dtype=torch.float32) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "p7pfnZbBllTg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7pfnZbBllTg",
        "outputId": "7a74c2ae-7d96-49a0-acdd-b5c1e9b63a69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1000, 12])"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "CzgjHKqnfAev",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzgjHKqnfAev",
        "outputId": "c0a2aea6-4a2c-41f8-84d0-bf1bb20941dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0,  0, 86, 85, 35, 77, 18, 52, 32, 10, 77, 84],\n",
              "        [ 0,  0,  0,  0,  0,  0,  0,  1, 25, 19, 42,  2],\n",
              "        [ 0,  0,  0,  0,  0,  1, 66, 88, 77, 88,  2, 66],\n",
              "        [ 0,  0,  0,  0,  6, 40, 34, 81, 83, 74, 59, 84],\n",
              "        [ 1,  0, 41, 49, 41,  2,  0,  0,  0,  0,  0,  0]], dtype=torch.int32)"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "tDBE780qfCkQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDBE780qfCkQ",
        "outputId": "8848b3f4-9b23-42fb-851f-5168537b2682"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 1., 1., 0., 1.])"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y85A_CpacQXJ",
      "metadata": {
        "id": "y85A_CpacQXJ"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7FUVHNOseqJv",
      "metadata": {
        "id": "7FUVHNOseqJv"
      },
      "source": [
        "We now need to build a model that will accept out weird sequence and try to classify it. We opt for a wickedly pared down variant of the transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "SBmVRdtT34V4",
      "metadata": {
        "id": "SBmVRdtT34V4"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "p2rWZzi84FFW",
      "metadata": {
        "id": "p2rWZzi84FFW"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A toy torch module for self attention. Informed by Andrej Karpathy's mingpt project\n",
        "    (https://github.com/karpathy/minGPT) and a slightly embarassing conversation with\n",
        "    ChatGPT-5 (https://chatgpt.com/share/690e21fa-e718-8004-860a-45109a95c291)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_dim=10, scale=False):\n",
        "        \"\"\"\n",
        "        Input dimensions are typically sharded across heads in multi-head attention.\n",
        "        We are aiming for simplicity and avoid this, using just a single 'head' with\n",
        "        the full input dimension.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.Q = nn.Linear(n_dim, n_dim)\n",
        "        self.K = nn.Linear(n_dim, n_dim)\n",
        "        self.V = nn.Linear(n_dim, n_dim)\n",
        "\n",
        "        self.n_dim = n_dim\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        We accept input in the shape of seq length, model dimension.\n",
        "\n",
        "        Note we don't need any linear layers as output because we're only using a single\n",
        "        attention head. If we had more, we would need to map our heads back into the d_model\n",
        "        space with a linear layer.\n",
        "        \"\"\"\n",
        "\n",
        "        # Project our input into the query space (i.e. multiply by the query weights),\n",
        "        # do the same for the key vectors. Then apply our similarity operation (dot product\n",
        "        # by way of matmul) to yield an attention tensor.\n",
        "        q = self.Q(x)\n",
        "        k = self.K(x)\n",
        "        attn = torch.matmul(q, k.transpose(-2,-1))\n",
        "\n",
        "        # We optionally scale our attention values down to avoid them blasting off and saturating\n",
        "        # the softmax function (thereby destroying gradients during backprop). For tiny models,\n",
        "        # this is probably not an issue and so we allow omission to simplify the model.\n",
        "        if self.scale:\n",
        "            attn = attn / math.sqrt(self.n_dim)\n",
        "\n",
        "        # Now normalize our logits with softmax so we can scale the value vector based on the\n",
        "        # attention we are learning to pay to each respective token\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        v = self.V(x)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "\n",
        "        return out, attn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "B1vWSfSafGpK",
      "metadata": {
        "id": "B1vWSfSafGpK"
      },
      "outputs": [],
      "source": [
        "class FaceClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, seq_len, n_dim):\n",
        "      \"\"\"\n",
        "      Define a model that accepts a sequence and predicts whether it\n",
        "      contains a face. We don't support a batch dimension to simplify the matmuls\n",
        "      etc.\n",
        "      \"\"\"\n",
        "      super().__init__()\n",
        "\n",
        "      # Give the model a scratch pad, project each character into a richer space to learn some features\n",
        "      self.embedder = nn.Embedding(vocab_size, n_dim)\n",
        "      self.attn = SelfAttention(n_dim=n_dim, scale=True)\n",
        "      self.fc = nn.Linear(seq_len * n_dim, 1)\n",
        "      self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, X):\n",
        "      \"\"\"\n",
        "      Do a forward pass through the network, applying attention and emitting a class probability.\n",
        "      We accept a sequence of single values (representing the respective character), of\n",
        "      shape (seq, 1).\n",
        "      \"\"\"\n",
        "      # Learn a multidimensional (n_dim size) representation of the input characters, one for each character\n",
        "      # (n_seq, 1) -> (n_seq, n_dim)\n",
        "      X = self.embedder(X)\n",
        "\n",
        "      # (n_seq, n_dim) -> (n_seq, n_dim)\n",
        "      X, attn_map = self.attn(X)\n",
        "\n",
        "      # (n_seq * n_dim) -> (1)\n",
        "      X = self.fc(torch.flatten(X))\n",
        "\n",
        "      # (1) -> [0, 1]\n",
        "      X = self.sigmoid(X)\n",
        "\n",
        "      return X, attn_map "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s_bNZNHuiApy",
      "metadata": {
        "id": "s_bNZNHuiApy"
      },
      "outputs": [],
      "source": [
        "def train(model, X_train, y_train, epochs=100):\n",
        "    \"\"\"\n",
        "    Train the face classifier\n",
        "    \"\"\"\n",
        "\n",
        "    # Use a fancy optimizer to help us converge, default learning rate\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    # Binary cross-entropy loss calculator for our binary classification task\n",
        "    loss_fn = nn.BCELoss()\n",
        "\n",
        "    loss_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # Training mode, ensure gradients are tracked, etc.\n",
        "        model.train()\n",
        "\n",
        "        # Forward pass\n",
        "        for x, y in zip(X_train, y_train):  \n",
        "            out, attn = model(x)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_fn(out, [y])\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Apply those gradients to inch, bound, fly towards lower loss\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_history.append(loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cthtKjHIhtUz",
      "metadata": {
        "id": "cthtKjHIhtUz"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bk7q9_AzhvjO",
      "metadata": {
        "id": "bk7q9_AzhvjO"
      },
      "source": [
        "Now let's split up our data and train the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "H4nL30GQhmAd",
      "metadata": {
        "id": "H4nL30GQhmAd"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "id": "baN7xKRohgIV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baN7xKRohgIV",
        "outputId": "f5e036ed-c28b-463a-b902-2594f7d824ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FaceClassifier(\n",
            "  (embedder): Embedding(90, 10)\n",
            "  (attn): SelfAttention(\n",
            "    (Q): Linear(in_features=10, out_features=10, bias=True)\n",
            "    (K): Linear(in_features=10, out_features=10, bias=True)\n",
            "    (V): Linear(in_features=10, out_features=10, bias=True)\n",
            "  )\n",
            "  (fc): Linear(in_features=120, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = FaceClassifier(vocab_size=len(face_charset), seq_len=seq_len, n_dim=10)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "ouPzsDzio1Rg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "ouPzsDzio1Rg",
        "outputId": "22145d94-bb42-4f04-86ba-ed612d16a28e"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[107]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, X_train, y_train, epochs)\u001b[39m\n\u001b[32m     21\u001b[39m out, attn = model(x)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m loss = \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Backprop\u001b[39;00m\n\u001b[32m     27\u001b[39m optimizer.zero_grad()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Local/school/590/mech-interp/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Local/school/590/mech-interp/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Local/school/590/mech-interp/venv/lib/python3.13/site-packages/torch/nn/modules/loss.py:727\u001b[39m, in \u001b[36mBCELoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m    723\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m    724\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    725\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    726\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m727\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Local/school/590/mech-interp/venv/lib/python3.13/site-packages/torch/nn/functional.py:3517\u001b[39m, in \u001b[36mbinary_cross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, reduce, reduction)\u001b[39m\n\u001b[32m   3515\u001b[39m     reduction_enum = _Reduction.get_enum(reduction)\n\u001b[32m   3516\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m target.size() != \u001b[38;5;28minput\u001b[39m.size():\n\u001b[32m-> \u001b[39m\u001b[32m3517\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3518\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) is deprecated. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3519\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure they have the same size.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3520\u001b[39m     )\n\u001b[32m   3522\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3523\u001b[39m     new_size = _infer_size(target.size(), weight.size())\n",
            "\u001b[31mValueError\u001b[39m: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size."
          ]
        }
      ],
      "source": [
        "train(model, X_train, y_train, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8J3yyB4QGrFd",
      "metadata": {
        "id": "8J3yyB4QGrFd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
